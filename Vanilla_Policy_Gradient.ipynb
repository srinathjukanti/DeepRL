{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla Policy Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUV6bZac0kj2Dxv71njGAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinathjukanti/DeepRL/blob/master/Vanilla_Policy_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6PikbQ_1xzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "import numpy as np\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pdb import set_trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7eSns-QBirT",
        "colab_type": "code",
        "outputId": "3e69df69-6701-4753-ed1c-6f605e86aee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Tensorboard Setup (Ngrok)\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "LOG_DIR = 'runs'\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-25 00:24:15--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.206.78.89, 34.197.28.250, 52.4.177.151, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.206.78.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  14.4MB/s    in 0.9s    \n",
            "\n",
            "2020-03-25 00:24:17 (14.4 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "https://5c112b3d.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWYwfJAb2Gzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, n_actions, device, lr, activation=nn.Tanh):\n",
        "    super(MLP, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.n_actions = n_actions \n",
        "    self.layers = nn.Sequential(nn.Linear(input_dim, 32), nn.Tanh(), \n",
        "                                nn.Linear(32, 64), nn.Tanh(),\n",
        "                                nn.Linear(64, n_actions))\n",
        "    self.device = device\n",
        "    self.lr = lr \n",
        "    self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    self.to(self.device)\n",
        "\n",
        "  def forward(self, obs):\n",
        "    obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "    scores = self.layers(obs)\n",
        "    return scores\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self, input_dim, n_actions, device, lr, gamma):\n",
        "    self.policy = MLP(input_dim, n_actions, device, lr)\n",
        "    self.device = device\n",
        "    self.gamma = gamma\n",
        "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "  def get_policy(self, obs):\n",
        "    logits = self.policy(obs)\n",
        "    return Categorical(logits=logits)\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "    return self.get_policy(obs).sample().item()\n",
        "\n",
        "  def compute_returns(self, episode_rewards):\n",
        "    R = 0\n",
        "    returns = []\n",
        "    for reward in episode_rewards[::-1]:\n",
        "      R = reward + self.gamma * R\n",
        "      returns.insert(0,R)\n",
        "\n",
        "    return returns\n",
        "\n",
        "  def compute_loss(self, obs, acts, returns):\n",
        "    logp = self.get_policy(obs).log_prob(acts)\n",
        "    t = -logp * returns\n",
        "    return (-logp * returns).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euo2uUsr2OtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(n_epochs=10, env_name='CartPole-v0', batch_size=5000, lr=1e-2, gamma=0.995):\n",
        "  env = gym.make(env_name)\n",
        "  input_dim = env.observation_space.shape[0]\n",
        "  n_actions = env.action_space.n\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  agent = Agent(input_dim, n_actions, device, lr, gamma)\n",
        "  tb = SummaryWriter()\n",
        "\n",
        "  def train_one_epoch():\n",
        "    batch_obs = []\n",
        "    batch_actions = []\n",
        "    batch_returns = []\n",
        "    batch_cumulative_rewards = []\n",
        "    episode_rewards = []\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "      action = agent.choose_action(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
        "      batch_obs.append(obs.copy())\n",
        "      batch_actions.append(action)\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      episode_rewards.append(reward)\n",
        "\n",
        "      if done:\n",
        "        batch_returns += list(agent.compute_returns(episode_rewards))\n",
        "        batch_cumulative_rewards.append(sum(episode_rewards))\n",
        "\n",
        "        obs, done, episode_rewards = env.reset(), False, []\n",
        "        if len(batch_obs) >= batch_size:\n",
        "          break;\n",
        "\n",
        "\n",
        "    agent.optimizer.zero_grad()\n",
        "    batch_loss = agent.compute_loss(torch.tensor(batch_obs).float().to(device),\n",
        "                                torch.tensor(batch_actions).float().to(device),\n",
        "                                torch.tensor(batch_returns).float().to(device))\n",
        "    batch_loss.backward()\n",
        "    agent.optimizer.step()\n",
        "\n",
        "    return batch_loss, batch_cumulative_rewards\n",
        "\n",
        "  for i_epoch in range(n_epochs):\n",
        "    batch_loss, batch_rewards = train_one_epoch()\n",
        "    tb.add_scalar('Epoch/Loss', i_epoch, batch_loss)\n",
        "    tb.add_scalar('Epoch/Average Reward', i_epoch, np.mean(batch_rewards))\n",
        "    print(f\"Epoch {i_epoch} \\t Loss {batch_loss} \\\n",
        "          \\t Average Reward {np.mean(batch_rewards)}\")\n",
        "\n",
        "  tb.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRFFbaNk_q6-",
        "colab_type": "code",
        "outputId": "d5be23d0-f8cc-4729-b23d-f0012f14f7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Loss 9.643745422363281           \t Average Reward 21.319148936170212\n",
            "Epoch 1 \t Loss 10.385102272033691           \t Average Reward 24.27403846153846\n",
            "Epoch 2 \t Loss 14.468417167663574           \t Average Reward 34.736111111111114\n",
            "Epoch 3 \t Loss 16.205259323120117           \t Average Reward 41.68595041322314\n",
            "Epoch 4 \t Loss 17.89388084411621           \t Average Reward 50.0\n",
            "Epoch 5 \t Loss 18.041746139526367           \t Average Reward 56.266666666666666\n",
            "Epoch 6 \t Loss 20.503896713256836           \t Average Reward 69.875\n",
            "Epoch 7 \t Loss 25.511638641357422           \t Average Reward 94.66666666666667\n",
            "Epoch 8 \t Loss 27.30763053894043           \t Average Reward 111.66666666666667\n",
            "Epoch 9 \t Loss 27.780227661132812           \t Average Reward 124.58536585365853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DNOsHe1_shK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dclznIKYATnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}